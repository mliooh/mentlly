{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# %% [code]\n\nimport os\nimport pickle\nimport torch\nfrom transformers import pipeline\n\ndef load_model(filename='model.pkl'):\n    \"\"\"Loads a model from a file if it exists, otherwise returns None.\"\"\"\n    if os.path.exists(filename):\n        print(\"Loading model from cache...\")\n        with open(filename, 'rb') as file:\n            model = pickle.load(file)\n        return model\n    return None\n    \ndef save_model(model, filename='model.pkl'):\n    \"\"\"Saves a trained model to a file.\"\"\"\n    print(\"Saving model to cache...\")\n    with open(filename, 'wb') as file:\n        pickle.dump(model, file)\nmodel = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ndef generate_response(prompt):\n\n    \n   \n    \n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a mental health chatbot that listens and provides helpful advice.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    prompt = model.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    outputs = model(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n    return outputs[0][\"generated_text\"]\n    # <|system|>\n    # I am Mently, your friendly mental health chatbot how can i help you today?\n    # <|user|>\n    # prompt</s>\n    # <|assistant|>\n    # response\n\n\ndef main():\n    \n    # load cached model\n    model = load_model()\n    #interface.launch()\n    print(\"I am Mently, your friendly mental health chatbot how can i help you today? \")\n    prompt = input(\"Propmt: \")\n    response = generate_response(prompt)\n    print(response)\n    \nif __name__ == \"__main__\":\n    main()\n            ","metadata":{"_uuid":"11070e3d-2dd9-475f-b0d8-40dd9521470f","_cell_guid":"5e85229c-c20b-4966-96c8-fd7f2e98aaa3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}
